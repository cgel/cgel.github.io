---
layout: post
title: Philosophy and AI
excerpt_separator: <!--more-->
---

I often find myself reading dissemination articles about AI. They talk about the potential and dangers this new technologies bring to the table. In one hand i am glad to see light shed to a topic that i care so much about, but sadly, this reporting often seems to come from an alternate reality, one closer hollywood that to the valley. And i can’t ignore an obvious pattern, the most misguided articles are written or heavily informed by philosophers.
<!--more-->
The first to realize the dangers of fission where physicists. The first to realize the dangers of gunpowder surely were the ones who invented it. But the first to realize the dangers of AI are philosophers. Sorry but no. It is obvious that these philosophers haven’t got a clue of what AI —or intelligence for that matter— actually is. They get lost in an imaginary world filled with false assumptions, and since they don’t have to come down to face reality —like implementing the ideas you have— they never discover those false assumptions. It is simple, you can’t understand physics just by thinking, so why would AI be any different?

If you are thinking that I’m just a philosophy hater you are wrong. I find philosophy genuinely interesting. I just don’t get confused on its nature. Philosophy is not a tool to discover the new or further the understanding of the old. It is a tool to motivate and inspire joyful intellectual curiosity. In short, philosophy is to intellectualism what syfy is to science.

Although i don’t consider AI to be an existential threat, i do believe that we shouldn’t completely ignore safety. What we really need is down to earth safety research by people that actually know the first thing about AI. An excellent example of this is this recent deepMind paper.
